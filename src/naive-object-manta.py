import concurrent
import itertools
from concurrent import futures
import networkx as nx
import pandas as pd
import pm4py

# Code was partly generated by use of Github Pilot.
# Assumption: partitioning set of events (to reduce combinatorial issue)
# only connected objects are considered for partitioning

THRESHOLD = 2

df = pd.read_csv('../data/BPI2017-Final.csv', sep=',', encoding='ISO-8859-1')
df = df.head(10000)
object_types = ["event_org:resource", "application", "offer"]
tuple_id = "event_id"
attributes = list(df.columns)
print(attributes)
time_attr = 'event_timestamp'

max_subset = None
max_value = 0


# get all unique values for each object type (attribute in dataset)
objects = []
for t in object_types:
    for o in df[t].dropna(axis="rows").unique():
            objects.append((o, t))

#print(objects)
G = nx.Graph()
for o,t in objects:
    G.add_node(o, label=t)
print("initialized graph")

# create graph
for ot1, ot2 in list(itertools.combinations(object_types, 2)):
    projected_df = df[[ot1, ot2]]

    # Filter rows where one of the columns is null
    filtered_df = projected_df.dropna(subset=[ot1, ot2])

    # Filter unique combinations of the column values
    #unique_combinations = filtered_df.drop_duplicates()

    for index, row in filtered_df.iterrows():
        if G.has_edge(row[ot1], row[ot2]):
            G[row[ot1]][row[ot2]]['weight'] += 1
        else:
            G.add_edge(row[ot1], row[ot2], weight=1)

# List of edges to remove
edges_to_remove = [(u, v) for u, v, data in G.edges(data=True) if data.get('weight', 0) < 1]

# Remove edges
G.remove_edges_from(edges_to_remove)

print("created graph")
connected_components = list(nx.connected_components(G))
print("computed connected components")
print(connected_components)


 def partitions(set_):
     if not set_:
         yield []
         return

     for i in range(1 << len(set_) - 1):
         parts = [[], []]
         for item in set_:
             parts[i & 1].append(item)
             i >>= 1
         for b in partitions(parts[1]):
             yield [parts[0]] + b
             # sort based on timestamps within partitions to emulate df relation?

 def dataframe_partitions(df):
     indices = df.index.tolist()
     for partition in partitions(indices):
         yield [df.loc[part] for part in partition]

 def sort_partitions(df, attribute):
     for partition in dataframe_partitions(df):
         sorted_partition = [part.sort_values(by=attribute) for part in partition]
         yield sorted_partition

 def subsets(partition):
     for r in range(len(partition) + 1):
         for subset in itertools.combinations(partition, r):
             yield subset

def k_partitions(df, k, attribute):
     all_partitions = list(sort_partitions(df, attribute))
     for subset in itertools.combinations(all_partitions, k):
         yield subset

# def derive_edges(partitions):
#     edges = set()
#     # Create a DataFrame that contains the next row for each row
#
#     for partition in partitions:
#         shifted_partition = partition.shift(-1)
#
#         # Iterate over both the current and next row
#         for (index, row), (_, next_row) in zip(partition.iterrows(), shifted_partition.iterrows()):
#             edges.add((row[tuple_id], next_row[tuple_id]))
#        # for row in partition.itertuples(index=True, name='Pandas'):
#        #     edges.add((row., partition[i + 1][tuple_id]))
#        #     print(f"Index: {row.event_id}, Timestamp: {row.event_timestamp}")
#     return edges
#
# def derive_edges_for_partition(partition):
#     edges = set()
#     indices = partition.index
#     for row in partition.itertuples(index=True, name='Pandas'):
#         print(f"Index: {row.Index}, A: {row.A}, B: {row.B}")
#         continue
#        # edges.add((partition[i][tuple_id], partition[i + 1][tuple_id]))
#     return edges
#
# def jaccard_sim(partitioning1, partitioning2):
#     edges1 = derive_edges(partitioning1)
#     edges2 = derive_edges(partitioning2)
#     return jaccard_sim_edges(edges1, edges2)
#
# def jaccard_sim_edges(edges1, edges2):
#     intersection = edges1.intersection(edges2)
#     if len(intersection) == 0:
#         return 0
#     return len(intersection) / (len(edges1) + len(edges2) - len(intersection))
#
# for partset in k_partitions(smalldf, THRESHOLD, time_attr):
#     edges = [derive_edges(part) for i, part in enumerate(partset)]
#     sum_sim = 0
#     for part in sort_partitions(smalldf, time_attr):
#        edges2 = derive_edges(part)
#        sim_values = [jaccard_sim_edges(e, edges2) for e in edges]
#        sum_sim += max(sim_values)
#     if sum_sim > max_value:
#         max_value = sum_sim
#         max_subset = partset
#
# def process_partition(partset):
#     edges = [derive_edges(part) for i, part in enumerate(partset)]
#     sum_sim = 0
#     for part in sort_partitions(df, time_attr):
#         edges2 = derive_edges(part)
#         sim_values = [jaccard_sim_edges(e, edges2) for e in edges]
#         sum_sim += max(sim_values)
#     return sum_sim, partset
#
# def batched(partitionings, batch_size):
#     #it = iter(iterable)
#     i = 0
#     batch = []
#     for part in partitionings:
#         print(++i)
#         batch.append(part)
#         # batch = list(itertools.islice(it, batch_size))
#         if len(batch) == batch_size:
#             yield batch
#             batch = []
#     yield batch
#
# #with futures.ThreadPoolExecutor() as executor:
# #    for batch in batched(k_partitions(smalldf, THRESHOLD, attribute), BATCH_SIZE):
# #        print(len(batch))
# #        future_to_partset = [executor.submit(process_partition, partset) for partset in batch]
# #        for future in concurrent.futures.as_completed(future_to_partset):
# #            sum_sim, partset = future.result()
# #            if sum_sim > max_value:
# #                max_value = sum_sim
# #                max_subset = partset
# #        print("Batch done")
#
# print("Max subset:", max_subset)
# print("Max value:", max_value)



