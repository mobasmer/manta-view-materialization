import concurrent
import itertools
from concurrent import futures

import pandas as pd
import pm4py

# Code was partly generated by use of Github Pilot.
# Assumption: partitioning set of events (to reduce combinatorial issue)

THRESHOLD = 2

def partitions(set_):
    if not set_:
        yield []
        return

    for i in range(1 << len(set_) - 1):
        parts = [[], []]
        for item in set_:
            parts[i & 1].append(item)
            i >>= 1
        for b in partitions(parts[1]):
            yield [parts[0]] + b
            # sort based on timestamps within partitions to emulate df relation?

def dataframe_partitions(df):
    indices = df.index.tolist()
    for partition in partitions(indices):
        yield [df.loc[part] for part in partition]

def sort_partitions(df, attribute):
    for partition in dataframe_partitions(df):
        sorted_partition = [part.sort_values(by=attribute) for part in partition]
        yield sorted_partition

def subsets(partition):
    for r in range(len(partition) + 1):
        for subset in itertools.combinations(partition, r):
            yield subset

def k_partitions(df, k, attribute):
    all_partitions = list(sort_partitions(df, attribute))
    for subset in itertools.combinations(all_partitions, k):
        yield subset

def derive_edges(partitions):
    edges = set()
    # Create a DataFrame that contains the next row for each row

    for partition in partitions:
        shifted_partition = partition.shift(-1)

        # Iterate over both the current and next row
        for (index, row), (_, next_row) in zip(partition.iterrows(), shifted_partition.iterrows()):
            edges.add((row[tuple_id], next_row[tuple_id]))
       # for row in partition.itertuples(index=True, name='Pandas'):
       #     edges.add((row., partition[i + 1][tuple_id]))
       #     print(f"Index: {row.event_id}, Timestamp: {row.event_timestamp}")
    return edges

def derive_edges_for_partition(partition):
    edges = set()
    indices = partition.index
    for row in partition.itertuples(index=True, name='Pandas'):
        print(f"Index: {row.Index}, A: {row.A}, B: {row.B}")
        continue
       # edges.add((partition[i][tuple_id], partition[i + 1][tuple_id]))
    return edges

def jaccard_sim(partitioning1, partitioning2):
    edges1 = derive_edges(partitioning1)
    edges2 = derive_edges(partitioning2)
    return jaccard_sim_edges(edges1, edges2)

def jaccard_sim_edges(edges1, edges2):
    intersection = edges1.intersection(edges2)
    if len(intersection) == 0:
        return 0
    return len(intersection) / (len(edges1) + len(edges2) - len(intersection))

df = pd.read_csv('../data/BPI2017-Final.csv', sep=',', encoding='ISO-8859-1')
smalldf = df.head(10)
tuple_id = "event_id"
attributes = list(df.columns)
print(attributes)
attribute = 'event_timestamp'
BATCH_SIZE = 1
max_subset = None
max_value = 0

for partset in k_partitions(smalldf, THRESHOLD, attribute):
    edges = [derive_edges(part) for i, part in enumerate(partset)]
    sum_sim = 0
    for part in sort_partitions(smalldf, attribute):
       edges2 = derive_edges(part)
       sim_values = [jaccard_sim_edges(e, edges2) for e in edges]
       sum_sim += max(sim_values)
    if sum_sim > max_value:
        max_value = sum_sim
        max_subset = partset

def process_partition(partset):
    edges = [derive_edges(part) for i, part in enumerate(partset)]
    sum_sim = 0
    for part in sort_partitions(df, attribute):
        edges2 = derive_edges(part)
        sim_values = [jaccard_sim_edges(e, edges2) for e in edges]
        sum_sim += max(sim_values)
    return sum_sim, partset

def batched(partitionings, batch_size):
    #it = iter(iterable)
    i = 0
    batch = []
    for part in partitionings:
        print(++i)
        batch.append(part)
        # batch = list(itertools.islice(it, batch_size))
        if len(batch) == batch_size:
            yield batch
            batch = []
    yield batch

#with futures.ThreadPoolExecutor() as executor:
#    for batch in batched(k_partitions(smalldf, THRESHOLD, attribute), BATCH_SIZE):
#        print(len(batch))
#        future_to_partset = [executor.submit(process_partition, partset) for partset in batch]
#        for future in concurrent.futures.as_completed(future_to_partset):
#            sum_sim, partset = future.result()
#            if sum_sim > max_value:
#                max_value = sum_sim
#                max_subset = partset
#        print("Batch done")

print("Max subset:", max_subset)
print("Max value:", max_value)



